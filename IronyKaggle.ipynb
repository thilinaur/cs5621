{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install backend\n",
    "import pandas as pd\n",
    "import pprint\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from string import digits\n",
    "from autocorrect import spell\n",
    "import preprocessor as p\n",
    "import requests\n",
    "import json\n",
    "import wordsegment as ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stemmer = PorterStemmer()\n",
    "ws.load()\n",
    "p.set_options(p.OPT.HASHTAG, p.OPT.NUMBER, p.OPT.EMOJI, p.OPT.SMILEY)\n",
    "def format_tweet(row):\n",
    "    \n",
    "    p.set_options(p.OPT.HASHTAG, p.OPT.NUMBER, p.OPT.EMOJI, p.OPT.SMILEY,p.OPT.URL, p.OPT.MENTION )\n",
    "    parsed_tweet = p.parse(row)\n",
    "    extra = []\n",
    "    if parsed_tweet.urls:\n",
    "        for each in parsed_tweet.urls:\n",
    "            row = row[:each.start_index]+ ' URL '+row[each.end_index:]\n",
    "            \n",
    "    if parsed_tweet.mentions:\n",
    "        for each in parsed_tweet.mentions:\n",
    "            row = row[:each.start_index]+ ' MNTN '+row[each.end_index:]\n",
    "            \n",
    "    if parsed_tweet.hashtags:\n",
    "        for each in parsed_tweet.hashtags:\n",
    "            extra+=ws.segment(each.match)\n",
    "    #print(row)\n",
    "            \n",
    "    if parsed_tweet.numbers:\n",
    "        for each in parsed_tweet.numbers:\n",
    "            extra.append('NMBR')\n",
    "    \n",
    "    if parsed_tweet.hashtags:\n",
    "        for each in parsed_tweet.hashtags:\n",
    "            extra.append('HASH')\n",
    "            \n",
    "    if parsed_tweet.emojis:\n",
    "        for each in parsed_tweet.emojis:\n",
    "            extra.append('EMOJI')\n",
    "    \n",
    "    if parsed_tweet.smileys:\n",
    "        for each in parsed_tweet.smileys:\n",
    "            extra.append('SMIL')\n",
    "    \n",
    "    p.set_options(p.OPT.URL,p.OPT.MENTION,p.OPT.NUMBER)\n",
    "    row = p.clean(row)\n",
    "    tokens = tknzr.tokenize(row)\n",
    "    tokens_formatted = []\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "           if tokens[i] in emoji.UNICODE_EMOJI:\n",
    "                tokens[i] = emoji.demojize(tokens[i])\n",
    "    \n",
    "    for each in tokens:\n",
    "        if each not in stop_words and len(each)> 0:\n",
    "            tokens_formatted.append(each)\n",
    "\n",
    "    tokens_formatted+=extra\n",
    "    return \" \".join(tokens_formatted).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(row_data):\n",
    "    data = []\n",
    "    i=0\n",
    "    for row in row_data:\n",
    "        i+=1\n",
    "        data.append(format_tweet(row))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "classes = []\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "\n",
    "with open('/Users/thilinara/IronyKaggle/train_edited.csv', 'r') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for row in spamreader:\n",
    "        try:\n",
    "            classes.append(int(row[1]))\n",
    "        except:\n",
    "            continue\n",
    "        data.append(row[2])\n",
    "traing_data = format_data(data)\n",
    "i=0\n",
    "for cl,d in zip(classes,traing_data):\n",
    "    i+=1\n",
    "    print(i,\" \",cl,\" : \",d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_test_set_row = []\n",
    "with open('/Users/thilinara/IronyKaggle/test_edited.csv', 'r') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for row in spamreader:\n",
    "        try:\n",
    "            row = row[1]\n",
    "        except Exceptions as ex:\n",
    "            print(ex)\n",
    "            continue\n",
    "        submission_test_set_row.append(row)\n",
    "submission_test_set_formatted = format_data(submission_test_set_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(ngram_range=(1,3\n",
    "                                         ), min_df=2)\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(traing_data)\n",
    "X_train_counts.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "#print(X.shape)\n",
    "pca = PCA(n_components=32)\n",
    "result_pca = pca.fit_transform(X_train_tfidf.toarray())\n",
    "print(result_pca)\n",
    "print(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traing_transformed = X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_accuracy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = 3400\n",
    "svd_transformed =traing_transformed\n",
    "X_test = svd_transformed[fraction:]\n",
    "Y_test = classes[fraction:]\n",
    "X_train = svd_transformed[:fraction]\n",
    "y_train = classes[:fraction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_lr = LogisticRegression(C=1,solver='saga',class_weight='balanced' )\n",
    "clf_lr.fit(X_train, y_train)\n",
    "print(clf_lr.score(X_train,y_train))\n",
    "t1 = clf_lr.score(X_test,Y_test)\n",
    "if record_accuracy:\n",
    "    v1=t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_rdf = RandomForestClassifier(n_estimators=500,max_depth=6,random_state=40)\n",
    "\n",
    "clf_rdf.fit(X_train, y_train)\n",
    "print(clf_rdf.score(X_train, y_train))\n",
    "t2=clf_rdf.score(X_test,Y_test)\n",
    "if record_accuracy:\n",
    "    v2=t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_nb = MultinomialNB()\n",
    "alpha=[]\n",
    "for i in range(1,1000):\n",
    "    alpha.append(i/1.0)\n",
    "parameters = {'alpha':alpha}\n",
    "search = GridSearchCV(clf_nb, parameters, cv=5)\n",
    "clf_nb = search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(clf_nb.score(X_train, y_train))\n",
    "t3=clf_nb.score(X_test,Y_test)\n",
    "if record_accuracy:\n",
    "    v3=t3\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_submission_counts = count_vect.transform(submission_test_set_formatted)\n",
    "submission_transormed=tfidf_transformer.transform(X_submission_counts)\n",
    "\n",
    "predicted_clf_rdf = clf_rdf.predict(submission_transormed)\n",
    "predicted_clf_lr = clf_lr.predict(submission_transormed)\n",
    "predicted_clf_nb = clf_nb.predict(submission_transformed)\n",
    "\n",
    "rs_sub = []\n",
    "for p in zip(predicted_clf_lr):\n",
    "    rs_sub.append(int(p))\n",
    "    \n",
    "predicted=rs_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('rs.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(['Index','Label'])\n",
    "    for i in range(len(predicted)):\n",
    "        writer.writerow([i+1,predicted[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
